\chapter{Discussion and Future Directions}
\label{ch:discussion}

This chapter briefly reviews the approaches taken in this work and then points to several ideas that are may provide the basis of viable extensions to the presented work.

First of all there are several remarks to be made about the experimental setup as presented in the previous chapter. The experiments executed to test the viability of the methods used in the wePorter system, suffer from a pivotal shortcoming in their design. Although 51 subjects is represents a reasonable amount of participants, the large amount of separate video segments causes each part to be interacted with only a handful of times in the current experimental setup. Although a case can be made that a scale-up is within reach considering the vast amount of user interactions received by online video platforms nowadays, it would have been wise to focus on a smaller set of content in order to acquire more telling data for each point.

Notwithstanding this issue that may be tough of as limiting to the results, it is actually very insightful to look at the results from the limited interaction data. Not only do the proposed filtering methods pick up salient segments in larger source videos, the selected shots are also evaluated as relatively preferable over shots that would be filtered out due to low focus rates. These quick results after a small number of interactions of human computers, might be attributed to the nature of the raw, unedited user-generated video content used in the experiment. By aggregating focus data over all users collectively, we might be able to filter an a high level between parts of content that are `exciting to watch' and `not worth my time'. The validity of these ideas should of course be tested in future work. 

Besides the extension of acquiring more user contributed data per video segment, another lead that would be very interesting to follow is iterative application of filtering methods, possibly to eventually leading to a convergence to a meaningful reconfiguration of the most salient content.

% at YT viewed a lot
The videos used for the main experiments have a total length of 25 minutes and their individual view counts range from 28 to 2304. Using the length of individual videos and the number of times they have been viewed, we have calculated that the total time spent on watching the 10 videos above, amounts to over 185 hours. That is a lot of user interaction that could help in the computation of the most interesting video segments. If directed through our one minute parallel play interface, all this user interaction would result in more than 11000 complete interactions. With 12 video parts presented in each interaction, it would amount to more than 130000 attentional ratings for video parts or over 900 ratings per part. 

% filtering can be applied
This is not yet regarding the system's filtering functionality. Once reliable estimates of user interest have been established from the collection of captured attentional ratings, we can filter for the parts that seem most interesting, that way enabling a convergence towards the most interesting video segments. A simple way of doing this would be to rank all video parts according to their average attentional rating and discard the parts that are systematically less attended to, on the assumption that they are considered less interesting. More complex procedures are of course possible, for example taking into account the number of ratings a part has received, the positions in sequence at which parts have been presented or the kind of users that have submitted the interactions (e.g. first time users versus experienced users). Once the initial set of source videos has been filtered, more data can be acquired for the remaining set, after which filtering can be applied again. This iterative process of filtering can continue until the process yields a small subset of video part that have acquired most attention in aggregate.

% if filtering applied
Filtering video parts to converge to a subset of the source videos that is  iteratively narrowed down, means that most interactive computation will be focussed on the video parts that receive most attention. There is an obvious issue of exploration versus exploitation here. When is filtering applied and what part of the current set of content is discarded in an iteration. Related to this filtering is the meaningful reconfiguration of segments into a new assemblage. Data-driven segment reconfiguration is currently left unexplored and it would be very exciting to see it investigated in the future.

% higher resolution of interest elicitation
Besides conceptual extensions the proposed framework, there are numerous variations possible in the current implementation that would be good to explore. In its current version the system uses aggregate counts for the time a user focussed on a particular sub-clips. Since the interactive player is set up to capture users' focus every duration $\delta$ (currently 100 milliseconds), this could also become the resolution of recording. That way each video parts would have $length/\delta$ time bins in a focus histogram. Another variation that will most likely yield exciting experiments, is to increase the number of videos displayed in parallel. Other potential variations include overlapping segment windows, using differently sized segment durations, the use of different video part durations within a single sequence.