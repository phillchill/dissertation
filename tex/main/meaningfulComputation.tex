\chapter{The Quest for Meaning in Video}
\label{ch:quest}

\section{Forms of Meaning in Film and Video}

\subsection{Meaning in Visuals}
[Video structuring, indexing and retrieval based on global motion wavelet coefficients]\cite{Bruno:2002tt}


\subsection{Meaning in Concept}
\subsection{Meaning in Structure}
[cite bordwell \& Thompson: analysis of context dependency]

\subsection{Meaning in Annotations}
[Telop-on-demand: Video structuring and retrieval based on text recognition]\cite{Kuwano:2000wy}
[Addressing the Challenge of Visual Information Access from Digital Image and Video Libraries]\cite{Christel:2005td}

\section{Computational Undertakings of the Quest}
\subsection{The Semantic Gap}


\subsection{Steps towards Meaning: An Overview}

Schematized summary of different steps:
indexing
automatic metadata
annotating
  Human-Driven Labeling
  Machine-Driven Labeling
multimodal feature fusion
concept based
content based [Relevance feedback: A power tool for interactive content-based image retrieval]\cite{Rui:1998uj}
[The Relative Effectiveness of Concept-based Versus Content-based Video Retrieval]\cite{Yang:2004tc}

[collaborative filtering] \url{http://en.wikipedia.org/wiki/Collaborative_filtering}

\subsection{Indexing to enable search}
Because visual data on it's own provides little machine-readable handles to search and find, repositories of multimedia content need to be index to enable search. Within the task of video indexing several approaches are taken 




