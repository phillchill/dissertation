\chapter{Human Computed Stories in wePorter}
\label{ch:weporter}



% [intro: sketch wePorter scenario. YouTube: Burning Man 1.7 million, Burning Man 2012 ..., Burning Man this month ...]
% 
% [discuss: is wePorter human computation?
% is it solving a task that would otherwise be done by computers? By humans? Can it be expected to ever be done by computers? Looking at answers from the questionnaire to the question of why users decided to change their focus, reveal different reasons for their behaviour. A big part of the answers are generic visual or auditory features like the presence of bright colours, video or audio quality and sudden changes in movement of the scenery of camera. These are all aspects of a video that can be computed computationally, some day perhaps even in a realtime online environment. The question whether or not other features such as curiosity or boredom are computable or not is beyond the scope of this thesis, but participants' response hint at the possibility that their interest is at least partially explicable by computable factors.
% 
% 
% plot aggregated reasons to change:
% 
% cinematography
%   camera movement
% content based
%   action
%   trees, boats
% visual
%   colour
%   quality
%   changes, movement
% audio
%   music
% Interest, curiosity / boredom
% unpleasant
%   sound
%   visuals
% ]

\section{Introduction}

% [TODO]

% The first part of definition of Human Computation included in chapter \ref{ch:human_computation} introduces an interesting question whether storytelling is a computable process.

This section describes the interaction design of the wePorter system, built to exemplify how the paradigm of human computation can be used in tasks like filtering of video segments and semi-automated video reconfiguration. The wePorter system runs an interactive webpage that functions as the main source for data acquisition, presentation of results and general proof of concept. In this chapter the system is analysed along the axes of \textit{purpose}, \textit{motivation} and \textit{task}, that were introduced in the analysis of Human Computational systems in chapter \ref{ch:human_computation}. Next to these guidelines for analysis, some remarks are made about the specifics in the functioning of the system. Lastly we discuss the implementation of the web application that is central in wePorter.

\section{User Generated Video Content}
\label{sec:weporter_ugvc}

 
%\url{http://YouTube-global.blogspot.co.uk/2009/05/zoinks-20-hours-of-video-uploaded-every_20.html}
% 
% \url{http://YouTube-global.blogspot.co.uk/2010/03/oops-pow-surprise24-hours-of-video-all.html}
% 
% \url{http://YouTube-global.blogspot.co.uk/2010/11/great-scott-over-35-hours-of-video.html}
% \url{http://YouTube-global.blogspot.co.uk/2011/05/thanks-YouTube-community-for-two-big.html}
% \url{http://YouTube-global.blogspot.co.uk/2012/01/holy-nyans-60-hours-per-minute-and-4.html}

\begin{quote}
  Since the dawn of YouTube, we’ve been sharing the hours of video you upload every minute. In 2007 we started at six hours, then in 2010 we were at 24 hours, then 35, then 48, and now...60 hours of video every minute, an increase of more than 25 percent in the last eight months. In other words, you’re uploading one hour of video to YouTube every second. Tick, tock, tick, tock — that’s 4 hours right there!
\end{quote}

These astonishing figures of the amount of video that is uploaded to YouTube are nothing short of mind blowing, but will most likely sound dated in a matter of years or even months. Looking at the increase of content uploaded to the video platform in past years, the growth does not seem likely to come to a halt soon [ref table]. All these videos are great for online video junkies, and are increasingly part of the online journalism landscape \cite{Rosenstiel:2012vb}. At the same time, all these videos being put online beg the question which ones of them to watch.

[graph of YouTube content uploads]

The increasing amounts of user-generated video content (UGVC) being put online, lead to an information overload and present both challenges and opportunities in search, retrieval\cite{Ulges:2009ua} and recommendation\cite{Zhou:2010ut} tasks [more refs]. There is an increased need for ways of aggregation and filtering. Both of these tasks rely heavily on an at least a shallow understanding of what is presented in these media, which, as we've seen in chapter \ref{ch:meaning_in_video}, is a hard problem to solve via current computational techniques. With so much content being uploaded, how can we find our way in the already enormous ocean of online videos?

\section{The Purpose}
\label{sec:weporter_purpose}

% Searching -> IR
With more than an hour of new content per second it is no wonder that YouTube has come to be viewed as the go-to for online video, much like ``the digital video repository for the Internet'' \footnote{\url{http://YouTube-global.blogspot.co.uk/2005/07/greetings-everyone-thanks-for-visiting.html}} that was envisioned by its founders in their first official blog post. An important activity on video platforms like YouTube is searching and much work has focussed on video retrieval\cite{Liu:2007fv,Snoek:jf,Christel:2005td,Halvey:2007tz,Kankanhalli:2008uv,Hollink:2005ei,DeRooij:2008ut,Snoek:2009dq,Smeaton:2008go}. We review the different aims in users' interaction in video retrieval tasks and point to an aspect of video retrieval that has not received much attention in recent research. This is the challenge of segmented video recommendation and we will explain why it forms a problem task to address within a Human Computation System like wePorter.

\subsection{Different Aims}
% [TODO change wording "goals" to "Aims", ref \cite{Smeulders:2000tx}]
Annotations reflecting the content of a video can, along with other meta data of the video, be used for retrieval of videos in response to textual queries[ref]. The effectiveness of such a retrieval task varies depending on the information that is used in the search algorithm[refs] and the type of content that is searched for \cite{Hollink:2005ei}[more refs]. A third characteristic that determines the effectiveness of a video retrieval system is the goals that users have in their usage of the system \cite{Hollink:2005ei}. User goals can vary widely from more to less specific\cite{Davidson:2010tu}. We expand on this latter point, as it forms an important context for the wePorter system.

\subsubsection{Direct Navigation}
The most specific goal is exemplified by a user who is drawn to a video platform by a direct link from an external website. Links can either be in the form of actual hyperlinks or playable embedded videos that are followed through to the platform. Navigations via such links form a direct mapping between a user's intention to the desired piece of content. In this case, users have a very specific reason to come and watch. Their desire, at least of knowing the contents of the video, is satisfied after the viewing. YouTube's system engineers call this way of video viewing \emph{direct navigation}\cite{Davidson:2010tu}.

\subsubsection{Search and Goal-oriented browse}
When users have not obtained a direct link to a potentially relevant piece of content, they might still have a specific goal in mind when visiting an online video platform. Reasons to visit might be the wish to see a particular music video or to find an instance of a series by a particular producer. This goal of discovering a rather specific video is referred to as \emph{``search and goal-oriented browse''}\cite{Davidson:2010tu}. Provided that the desired piece of content exists and the video platform has an appropriate search function in place, these `narrow queries', will result is a result set of search results from which the user is likely to handpick the sought-after result fairly quickly. Here the user's desired result often lies within a single item of content. Perhaps a few misses are required, but after a couple of clicks the user hits the desired video.

\subsubsection{Unarticulated Want}

% 
Yet a less specific goal is seen in users who come to a video platform ``to just be entertained by content that they find interesting'' \cite{Davidson:2010tu}. These users mainly browse from one piece of content to the next, often aided by the platform's recommendations of related content. It has been found that YouTube's related video recommendation functionality, which recommends videos that are related to the video currently being watched, is one of the most important view sources of videos. In fact, traffic received from these recommendations is the main source of views for the majority of videos on YouTube \cite{Zhou:2010ut}. Features derived from users' navigations such as `click-through rate' have been used to improve content-based video recommendation \cite{Yang:2007vb}.

Goal of a person's query in this kind of navigation is no longer defined in a single returnable item of content or even a containable set of items. Rather, the interactive pathway through the a set of interesting bits of content is what represents a user's aim. This broader, exploratory goal of finding different parts of interesting content has been termed `unarticulated want'\cite{Davidson:2010tu}.

\subsubsection{Encapsulated Wander}
% TODO ref \cite{Smeulders:2000tx} compare to `association based search' + "most hampered by the semantic gap""
Considering the three categories of user motivation above, another, composite motive can be thought of. Users often start with a query for a particular topic, followed by a journey across many videos relating to their search term. Their navigation seems unarticulated but it is encapsulated by the topic of their query. Think of someone who wants to get an overview of a large music festival she recently attended. Big events where many people record videos, are often massively covered on UGVC platforms, resulting in an overload of visual information. Searching YouTube for this month's videos from the participatory festival Burning Man, two weeks after it ended, returns ``About 7,660 results''\cite{youtubeBM}. A similar large set of topically related UGVC can be imagined at a website that asks participants to contribute their videos recorded at a recent event or centred around a particular topic.

This kind of `broad queries' returns a result set of related content in which a user will probably consider many items as a successful retrieval. Furthermore, one could even say that the desired result of a user's query is spread across the multiple pieces of content. By traversing the space of different videos in the result set, users interactively construct the desired answers to their own queries. We call this motivation for discovery within a topically-related set of videos `encapsulated wander'.

% TODO link these two paragraphs
% interactivity in search
Interactivity is generally agreed to play an important role in the task of video retrieval, as is reflected by the separate category in the annual TRECVid challenge for interactive video retrieval\cite{Smeaton:2006ww}. Several works have indicated the importance of interactivity in the task of video retrieval to filter through a set of initially returned results \cite{DeRooij:2007ua, Christel:2004wm, DeRooij:2007ua, DeRooij:2008ut}. While most of these systems are aimed at retrieval of clearly specified queries, exemplified by the TRECvid retrieval task, the need for interactive exploration is even more apparent for the broader oriented goal of users engaged in `encapsulated wander'.

\subsection{Serving the Purpose of Encapsulated Wander}
% goal of retrieval = journey through content
The answer to a user's query now lies as much in the journey through the content as in the returned content itself. By traversing from one piece of content to the next, users construct a sequence of concatenated items. This self-constructed story is an important concept that wePorter capitalises on, as will soon become apparent.

% journey = 1. multiple items
The task at hand of recommending a larger group of interesting videos is radically different compared to the more narrow queries that could be answered by a small set of true positives in an information retrieval task. Besides the spread of the searched for result across different pieces of content, there is a second important difference that lies in the nature of the majority of UGVC.

% journey = 2. sub-clips
% TODO make shorter
User-contributed videos commonly consist of raw, unedited footage. In \cite{Rosenstiel:2012vb} Rosentiel and Mitchell report that within the collection they investigated only 39\% of the news-related footage contributed by citizens was edited. It should be noted that this collection contained only the most popular videos per week and that a different distribution will be found in the complete set of news-related videos or all the videos hosted on YouTube. 

Users with broad expectations will not only want to be presented with multiple relevant items from a complete repository, they are also looking for the most interesting parts within these relevant items. This issue is particular to time-based media, and especially relevant for video. Other temporal media, like audio in general and music in particular, have less of a need for segmentation because of  their common usage in multimedia applications. People usually tend to listen to a song entirely and if they which to experience an album in part, constituent songs are already units on their own that can easily be reconfigured. Tag-a-tune is a game with a purpose used to acquire tags for clips of music. Although it could be employed for labelling of smaller audio sub-clips within songs, the games only aims at global labelling of a sounds\cite{Law:2009vl}.

Because of the unstructured nature of the majority of UGVC it is desirable to establish local recommendations that point to `sub-clips' within a video that are of particular interest. Whereas digital music albums shared online consist of a collection of songs that can each easily be made to stand alone, video currently suffers from a less malleable identity online.  Online videos are currently much like black boxes that can be played, paused, rated, commented on, tagged and shared only in its entirety. What if a piece of raw, unedited UGVC features something spectacular for ten seconds halfway along its timeline, but shows much of the same for the rest of the time? Answering this question will be the first part of the purpose of the wePorter system.

\subsection{Storytelling as Structured Recommendation}

% explanation why it is also good to structure the filtered pieces of content into a new configuration.
[TODO; based on section in Meaning chapter. Indicate necessity for segmented recommendation + segmented annotation in general. Point to little attention to the retrieval of specific video segments \cite{Snoek:2009dq}\cite{Kankanhalli:2008uv}.]

% concluding Purpose
The ten significant seconds in a two-minute video become a needle in a haystack when an initial set of videos relating to your query includes hundreds to thousands possibly relevant videos with lengths between some tens of seconds and a couple of minutes. The aggregation and reconfiguration of several of these `needles' into a meaningful new whole is another non-trivial task. We present wePorter as a test case for new methods that address both these issues of information overload in video libraries of UGVC. More precisely, wePorter's purpose is two-folded:

From a set of topically related unstructured user-generated videos:
\begin{enumerate}
  \item Filter localised intervals (in time) of global interest (across multiple users) within each video within a set of source videos.
  \item Reconfigure interesting video segments into a meaningful new story
\end{enumerate}		

In order to find local regions of global interest in videos, we assume there is a correspondence in the judgements of different people as to what parts of a video are most interesting. This assumption will not hold for video content in general, but might be reasonable for the unstructured UGVC we address in wePorter. Many videos that belong to this category have intervals where a particular action or event takes place, surrounded by additionally recorded material featuring little activity that is not edited out. One of the intensions of wePorter's current implementation is to see whether distinctions can be made between on this coarse level of preference.

\section{The Motivation}
\label{sec:weporter_motivation}

How to get a group of unrelated people to contribute their efforts to solving the tasks set in our two-folded purpose? This section looks at the reasons people might have to contribute their computational powers to a system with a purpose like wePorter. Looking at the way people engage with online video content on platforms like YouTube, we identify patterns in their behaviour that can be matched to a task in a human computation system. This behaviour that is characterised by a more active role in multimedia consumption, can be seen as a larger trend in the development of new media. The end of this section indicates how the motivations of users of the wePorter system can link in with this larger trend.

Jain and Hampapur indicate entertainment, information or communication as purposes for the creation of consumer videos \cite{Jain:1994we}. Although most UGVC is probably not as purposefully produced as the professional productions that Jain and Hampapur report on, these different purposes give an indication as to what user's motivations might be when interacting with a video system.

\subsection{Information Provision through Online Video}

% [ref \cite{Jain:1994we}]
Since the proliferation of mobile video recording devices, it has become common practice for large-scale (semi-)public events to be covered by UGVC that gets uploaded to the web. While some are critical\cite{Jonsson:2011fh} to the often heralded democratisation and empowerment of people by the new media production and distribution tools, it is clear that the UGVC at places like YouTube attracts a lot of traffic from people looking to be informed about recent events. After all, UGVC can have its advantages over traditional media when it comes to video news coverage, especially for unexpected events where traditional media do not have the immediacy of user-generated `reports' recorded by coincidental passersby. 

In a recent study as part of the the Pew Research Center’s Project for Excellence in Journalism, the most popular video's from YouTube's `News and Politics' were analysed for a period of 15 months\cite{Rosenstiel:2012vb}. The authors of the study exemplify the power UGVC can have in news provision by showcasing frequently viewed videos detailing scenes from the earthquake and subsequent tsunami that hit Japan in March 2011. The week following the disaster, the 20 most viewed news-related videos on YouTube all related to the catastrophic event and were together viewed more than 96 million times. Most of these videos were recorded by individuals who happened to be in the affected areas when the disaster struck, either uploaded by themselves, or by TV channels who appropriated the content. The study furthermore reports that in the studied period, the most searched term of the month on the YouTube platform as a whole was a news-related event 5 out of 15 months.

While the journalism study above focusses on videos with the `News \& Politics' label, information provision about current events might span a larger set of categories. Someone looking for footage in order to get a sense of the atmosphere at a recent music festival or public demonstration, might very well find relevant videos in categories like `Entertainment', `Travel \& Events' or `Nonprofits \& Activism'. Across all of these categories, we are able to find examples of vast collections of UGVC, uploaded in the period following up newsworthy events.

\subsection{Informative Entertainment}

The wePorter system focusses on these kinds of topically related sets that people are currently exploring interactively by browsing from one video to the next. This way of navigation is an intermediary between the goals of \textit{goal-oriented browse} and \textit{unarticulated want}. . The apparently aimless browsing is now encapsulated by the event but users still roam freely within this topicalised set of content. By navigating from video to video, watching some and skipping others, users leave attentional traces that give valuable insight into a user's intentional standpoint.

It is this kind of interactions that are already taking place at a large scale that we like to make use of in the wePorter system. Motivated by the wish to explore informative content, users will instinctively and implicitly contribute their human knowledge to a system that is set up appropriately. This kind of motivation fits the category of `implicit work' as it involves activities that people already engage in for their own reasons \cite{Quinn:2011us}. Considering users' wish to be informed and the interactive way in which they navigate, there is most likely also a factor of entertainment involved though. We expect though that the more specific motivation of information provision might show to become a valid categorisation for the motivation of people in a HCS as it is a common activity on the web and inherently linked with the hard problem of meaningful interpretation of content.

\section{The Task}
\label{sec:weporter_task}

In this section we take a look at how the larger goal of finding intervals of regional interest across time within a single video can be branched out into bite-sized tasks executable by a person in a single interaction. We begin by introducing some conceptual considerations that influenced the interface design. Then a detailed overview of the wePorter web interface is presented. We end with a  section focussing on the implementation of the system.

\subsection{Design Considerations}
Below are included several points that have been instructive in the development of the interactive task central to the wePorter system. Some of these point are system requirements, others are more guiding design principles or thoughts that have been inspiring and formative in the development.

\subsubsection{wePorter is a web interface}
The power of a HCS that relies on data from many interactions is truly unleashed in an online setting, where many people can easily participate and interact. For this goal alone already, wePorter must be a web-based system.
Besides the obvious choice of staying in the realm of the online video content, it makes sense to embed the theoretical explorations of this research in the practicalities of current web technologies. With the ongoing development of technology like HTML5, many new possibilities for a user's web browser are unleashed. The implementation of a research tool concerning online video is a good opportunity for the exploration of the technological possibilities of present day web technologies.

\subsubsection{Hypervideo}
The power of digital content on the internet lies for a great part in its capacity to be hyperlinked. Linking to externally hosted content alleviates the burden of having to host or recompile pieces of media. Instead, files can be played and remixed by reference, leaving their respective sources intact and where they are. In the presentation of their digital video repurposing system `Diver', Pea et al. indicate the advantages of using a virtual camera controlled by XML-based files that reference parts of source video instead of rendering new video clips\cite{Pea:2004td}:

\begin{itemize}
  \item ``Virtual video clips eliminate the generation of redundant video files, greatly reducing disk storage requirements.''
  \item ``No rendering time means vastly improved performance. Users can instantly create and play back dynamic path videos without long video-rendering delays.''
\end{itemize}

An implementation of a system where users interact with content that is dynamically reconfigured in real-time will benefit considerably from a hyperlinked functionality, especially when this takes place in an online setting where bandwidth will be limited.

The idea of hypervideo in the context of interactive narratives has been proposed by  Sawhney et al., where users were invited to navigate a virtual cafe by means of \emph{temporal} and \emph{spatio-temporal} and \emph{textual} links present in the video interface. A temporal link is  ``[a]	time-based	reference	between different video scenes, where a specific time in the source video triggers the playback of the destination video scene''\cite{Sawhney:1996tk}. wePorter utilises temporal links to link sequences of video scenes together.

\subsubsection{Localised Interest} % Define `source videos' and `video parts' here?
To answer to the first purpose of wePorter, we wish to distinguish parts of videos based on their level of interest. In order to elicit users' preference for particular parts within a video, we divide each \emph{`source video'} in our initial set of topic-related content into smaller \emph{`video parts'} and present a selection of these in a user interaction. Slicing up source videos virtually and playing their parts by reference is made possible by a hyperlinked implementation of the video player.

\subsubsection{Implicit User Feedback}
% enjoyable interest elicitation
The user interaction design should enable means to learn about a user's interest in a video at a particular moment in time. This to the purpose of discovering localised regions of interest within separate videos. To make a user's interaction as enjoyable as possible, implicit data acquisition should be preferred over explicit questions. In other words, user will be more likely to repeat a task that implicitly logs their behaviour during interaction, than one where they are presented with a questionnaire after every click. By making the acquisition of user feedback an integral part of an interaction, users are directed into contributing their computational power without even being aware of it. In wePorter, this enables \emph{`curation through interaction'} by using the \emph{`implicit feedback'} to maintain and improve a dynamic story space.

\subsubsection{Capturing Attention Time by Parallel Play}

% attention -> interest
Considering measures that could indicate how people's interest varies across different videos, an idea that quickly surfaced is that interest is closely linked to attention. When a piece of content contains something that is interesting to many people, this will most likely result in an increase in views, provided the content is accessible to a variety of people. This simple notion is the idea behind global recommendations that show most popular or `trending' content. Whether the trending item is a video on a sharing platform or a phrase on a microblogging service, when there is a large number of people attending to it, this is a reason to suspect the item to be of interest for people who haven't engaged with it yet. 

% normally need for personalization
An obvious limitation of these global recommendations is the lack of personalisation. Personalised recommendations are offered because the content that is globally popular may not be related to the topics of my interest. For the purpose wePorter is serving however, focus around a particular topic is already in place and we are in the first instance mostly interested in picking out the parts that share a high level of interest globally. The experiments described in chapter \ref{ch:evaluation} are thus not focussed on personalisation. 

To state the our assumption about the relation between users' attention and interest more formally we adjust the formulation of Xu et al. to address global interest in video segments:

\begin{quote}
  For a video segment $i$ we denote the user $u_j$’s attention to it as $at(i, u_j)$, which is the time the user spends attending to the object. Let $i_1$ and $i_2$ be two video segments. Let's assume after they are both presented to and watched by the user $u_j$, we have $at(i_1,u_j) > at(i_2,u_j)$; then it is reasonable to infer that $u_j$ is more interested in $i_1$ than $i_2$.''
\end{quote}

The use of attention time for implicit user feedback is relatively new and its relation to interest and preference is still being investigated in research. Results of Kelly and Belkin are critical to the effectiveness of attention time as implicit feedback \cite{Kelly:2001ui,Kelly:2004tq}. Their work reports no ``significant relationships between display time and usefulness judgments for the subjects in [the] study''. Furthermore they warn about substantial differences in users attentional behaviour and the dependence of their behaviour on the specific task the user is involved in.


The assumption that attention time may indicate interest is used in \cite{Xu:2008vb} where methods of predicting a user's interest in online documents, are based on the time the user has spent attending to previous documents. Xu et al emphasise the need for personalisation in content recommendation as well as their method's advantage of allowing for implicit user feedback through the use of commodity eye-tracking.

% parallel play as a new method for interest elicitation
The wePorter system uses a method of user preference elicitation similar to that of \cite{Xu:2008vb}, that makes choice of attention an integral part of the user's task. User's make explicit choices in their attention but their feedback is captured implicitly in the interaction. A web interface presents a user with two concurrently playing videos for which we would like to elicit preference. The user is forced to make an explicit choice of attending to one video or the other. During this `parallel play' of two video parts we capture the amount of time attended to each of the parts ${at(i_1,u_j), at(i_2,u_j)}$ and store this for later analysis. Parallel play may prove useful especially in eliciting preference for time-based media like audio and video as the time users attend to an item can be easily incorporated in existing interactions, thus matching the motivation of `implicit work' mentioned in section \ref{sec:characterising_hcs}.

In the parallel presentation of to 
% We are not aware of other work using a similar method for preference elicitation of time-based media.
% TODO check these last 2 sentences

\subsubsection{Recurrent Interaction}
Because of the reliance on data, user's should be able (and encouraged) to engage in the interaction more than once.

\subsubsection{Users Between Consumers and Producers}
Studies reflecting on new media technology and its incorporation in our everyday life are in recent years often speaking of a media convergence, where multimedia content flows dynamically across multiple media platforms and media audiences take an active, participatory role in their search for entertainment experiences. In his book `Convergence Culture', Jenkins writes:

\begin{quote}
  ``This circulation of media content - across different media systems, competing media economies, and national borders - depends heavily on consumers' active participation. I will argue here against the idea that convergence should be understood primarily as a technological process bringing together multiple media function within the same devices. Instead, convergence represents a cultural shit as consumers are encouraged to seek out new information and make connections among dispersed media content. [...] The term \emph{participatory culture} contrasts with older notions of passive media spectatorship. Rather than talking about media producers and consumers as occupying separate roles, we might now see them as participants who interact with each other according to a new set of rules that none of us fully understands.''\cite{jenkins2006convergence}
\end{quote}

Surveying the diverse body of research into interactive TV, Cesar and Chorianopoulos propose a new way of looking the life cycle of digital content that considers content editing, content sharing and content control as an alternative to the more hierarchical `produce-deliver-consume' paradigm associated with traditional media\cite{Cesar:2009fk}. The movement from passive consumers to (inter)active contributors indicates new expectations by users of new media applications. The trend of users' more active engagement in new media technology fits well with the approach of recommendations based on user interaction and our proposal of storytelling as structured recommendation.

The idea of a user of a system taking both roles of consumer and creator is directly reflected in the system setup of wePorter. The idea is that users of the interactive web interface are interacting because they are motivated by a wish to be informed and entertained and at the same time contribute their computational powers in the form of attentional data that helps shape future stories. This way we could characterise the \emph{task-request cardinality} as many-to-many.

\subsubsection{The Death of the Author, the Birth of Collective Creation}
% intro
The first part of the title above was originally voiced by Roland Barthes in a 1968 essay bearing the phrase as its title, in which he contemplates the role of the author in literary writing\cite{Barthes:1977wq}. His title `the death of the author' points to his criticism to the importance that typically gets associated to the author in the analysis of literary work. To Barthes, a text is not ``the “message” of the Author-God'', but rather ``a multi-dimensional space in which a variety of writings, none of them original, blend and clash. The text is a tissue of quotations drawn from the innumerable centers of culture''. Instead of the author, for Barthes, it is the reader where the diverse backgrounds of text become unified. Barthes' criticism is resolute:

\begin{quote}
  ``the birth of the reader must be at the cost of the death of the Author''
\end{quote}

In our aim of reconfiguring interesting sub-clips into a new arrangement, the changing role of the author surfaces in a new context. Barthes' thoughts are relevant in two ways. First because he sees an author's work as the reconfiguration of previous works. Second because he assigns the reader (spectator, viewer, listener, etc) the important role of central interpreter. This links to the contextual nature of human perception discussed in section \ref{sec:challenges_in_interpretation} as well as the active role a reader takes in the construction of a coherent story. Explicit reconfigurations of video content may be a way to provide users with a basis from which they `enact' their own interpretation of a story or a depicted event.

% less restrictiv licensing -> remixing
(Co-)authoring by means of reconfiguration or \emph{`remixing'} is common practice in today's vivid content sharing communities \cite{Diakopoulos:2007jn} and an important factor in these relatively new forms of creation seems to be attribution\cite{Luther:2010wq}. Although classically intellectual property licences mostly have a restrictive function (protecting my intellectual property from being exploited), new less restrictive licences also offer a way of attribution in digital content that is derived from external sources. Less restrictive forms of digital content licensing, like Creative Commons (CC), mean that it is now possible for content uploaded by its original creator, to be used under specified conditions in a new piece of work by someone else. This kind of licences has been noted to be an important facilitator of research into Human Computation\cite{Law:2009vl}. They make it possible for works not only to be used and remixed by other individuals, but also to be incorporated in algorithmically constructed reconfigurations of user generated content.

% popularity CC
Different video platforms are currently offering less-restrictive CC licensing as an integrated part of their services. YouTube currently offers the option of choosing a most basic attribution licence and reports 4 million videos licensed this way \cite{YouTubeCC}. The video platform Vimeo\footnote{\url{www.vimeo.com}} focusses on letting video and animation producers share and showcase their original work. The platform has internalized the use of CC from 2010\cite{vimeoTimeline} and many of their users licence their videos such that they can be remixed by others. Figure \ref{fig:vimeoCC} shows that a large part of the licences on the Vimeo platform allow derivatives to be made\cite{vimeoCC}.

\begin{figure}[htbp]
  \centering
    \includegraphics[scale=1]{img/vimeoCC}
  \caption{Number of videos for each of the Creative Commons licences on Vimeo}
  \label{fig:vimeoCC}
\end{figure}

% collaboration user & software designer
Besides the collective actions of the multiple users that help shape the creation of new configurations of content, there is a further level of collaboration between the users and the tools they interact with. Manovich even extends this relation to the tools' designers in his view on collaborative new media authorship:

\begin{quote}
  ``Authoring using [Artificial Life] or [Artificial Intelligence] is the most obvious case of human-software collaboration. The author sets up some general rules but s/he has no control over the concrete details of the work – these emerge as a result of the interactions of the rules. More generally, we can say that all authorship that uses electronic and computer tools is a collaboration between the author and these tools that make possible certain creative operations and certain ways of thinking while discouraging others. Of course humans have designed these tools, so it would be more precise to say that the author who uses electronic/ software tools engages in a dialog with the software designers [...].''\cite{manovichAuthor}
\end{quote}


\subsection{The Interface}
\label{sec:weporter_interface} 
% intro
This sections describes the user interface that directs participation of wePorter users towards solving the purpose of distinguishing local intervals of interest within videos. After a conceptual overview of the functioning we include a walkthrough to explain precisely how the interaction takes place.

% [TODO] move to walkthrough or design considerations
% hypothesis: interest will be reflected in attention
We hypothesise that interesting parts of content will attract a relatively large amount of attention compared to less interesting parts. 

% parallel play; move up to design considerations?
To force a users to make an explicit choice between parts of content for which we would like to elicit their preference, we present two pieces of video playing concurrently and force users to attend to one or the other. In order to get an idea of the variation of interest across a video, \emph{`source videos'} are divided into smaller \emph{`video parts'}, each of which is presented separately in interactions over time. During this `parallel play' of two video parts we capture the amount of time attended to each of the parts and store this for later analysis. 

\subsubsection{A Walkthrough}
% arrival at webpage
When a user opens the wePorter web interface he is welcomed by a short introduction to the project and successively guided to further instructions explaining the experiment. The instructions as they are presented to the user are shown in figure \ref{fig:instructions}.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=\textwidth]{img/instructions}
  \caption{wePorter Instructions}
  \label{fig:instructions}
\end{figure}

% loading sequences
Upon loading the webpage, a database is queried for a pair of sequences made up from different video parts for which the system would like to elicit a user's preference. The two sequences both consist of six video parts that each have a duration of 10 seconds. The interaction of the two sequences playing in parallel thus has a total duration of 60 seconds, reflecting the short time span common across UGVC at YouTube\cite{Cha:2007ty, Cheng:2007tc}. A detailed description of the algorithm used for the construction of these sequences is given in section \ref{sec:weporter_implementation}.

% starting interaction
After reading the instructions, the user scrolls down to the interactive parallel video player that displays two videos on top of each other. By clicking the `Play' button, the user starts the interaction and sets in motion the consecutive playback of both sequences in parallel.

\begin{figure}
  \centering
  \begin{subfigure}[b]{.5\textwidth}
          \centering
          % \includegraphics[width=\textwidth]{img/pplay1}
          \includegraphics[height=10cm]{img/pplay1}
          \vfil
          \caption{Upon load}
          \label{fig:pplay1}
  \end{subfigure}%
  ~
    %add desired spacing between images, e. g. ~, \quad, \qquad etc. 
    %(or a blank line to force the subfigure onto a new line)
  \begin{subfigure}[b]{.5\textwidth}
          \centering
          % \includegraphics[width=\textwidth]{img/pplay2}
          \includegraphics[height=10cm]{img/pplay2_cursor}
          \vfil
          \caption{While playing, focus on top video}
          \label{fig:pplay2}
  \end{subfigure}
  \caption{The wePorter parallel play interface}\label{fig:interface}
\end{figure}


% focus
During the parallel play of the two sequences of video parts, the user triggers which of the two videos is in `focus' by placing the mouse cursor over it. When focus is placed on a video, this makes it audible and clearly visible. The unfocussed video is silent and still dimly visible. This partial visibility allows the user to discern to a limited extend what is displayed in the unfocussed video. Seeing something that attracts interest can lead the user to change focus from one video to the other. The limitation of only one of the two videos being in focus at once, gives users incentive to explore the narrative space of the parallel sequences. The aspect of focus lets users spread their attention between concurrent parts by:
\begin{itemize}
  \item making a choice to attend to a video part they find most interesting.
  \item changing from time to time to check what is being played in the unfocussed video.
\end{itemize}

We record which video a user is attending to by keeping a count for each of the two video parts playing concurrently and increasing the count for the focussed video every 100 milliseconds. When a video part ends, the count is logged internally on the user's browser side before the next video part is started with its own count. When the parallel sequences are played back completely, the end of the interaction is reached and the counts for each of the 12 video parts are stored in a server-side database. Each pair of counts for two concurrently presented parts, represents a distribution of the user's attention over those parts.

Note that users are never explicitly asked to point at the video that is most interesting. Users are simply instructed as to how the interface works and then left to explore the videos as they like. By recording users' behaviour this way, we achieve a detailed insight into which of a pair of videos a user has attended to at what time. In section \ref{ch:evaluation} we report on the merits of these measures for the processes of filtering and reconfiguration.

\section{Implementation}
\label{sec:weporter_implementation}

This section describes in detail the functionality of the wePorter system, the different components in use and the way they relate to each other. We begin by illustrating how video content is prepared for presentation in the wePorter web interface, and next describe the system framework. 

\subsection{Preparation of Content}

In order to get localised feedback on distinct temporal intervals within videos, we present users with a sequence of `video parts' of equal duration, each originating from their own respective `source video'. A initial step is thus to prepare video parts so they can be presented in a user interaction. What is played back to a user is a part of the source video referenced by hyperlinks to start and end points. This hyperlinked implementation means slicing up source videos does not involve cutting up video content or recompilation of any sort. 

\begin{figure}[htbp]
  \centering
    \includegraphics[width=.7\textwidth]{img/preparing_content}
  \caption{Slicing Source Videos into Video Parts by Reference}
  \label{fig:content}
\end{figure}

The wePorter system takes as a starting point a set of topically related source videos, representing a result set that could be acquired by querying a large UGCV platform for video from a large scale public event. We keep a database of source videos, storing their source path and length:

\begin{equation}
  video = (vIdx, srcPath, length)
\end{equation}

where $vIdx$ is the video's index. Next, we define video parts as tuples of source video and two time codes referencing start and end:

\begin{equation}
  videoPart = (vpIdx, vIdx, tc_{in}, tc_{out})
\end{equation}

where $tc_{in}$ references the time code within the video indexed by $vIdx$ that is the start of $videoPart$ and $tc_{out}$ references the time code within the video indexed by $vIdx$ that is the end of $videoPart$. 

% generate_video_parts Algorithm
Algorithm \ref{alg:videoparts} shows the procedure to generate video parts from source videos. The algorithm starts at the beginning of a video and extracts a video part for every consecutive window of duration $d$. 

% end bits of videos discarded
There might be an interval with a duration less than $d$ at the end of a source video that is not included in the resulting set of video parts. Because the parallel sequence player expects video parts of equal length, these end bits are discarded and will not be presented during user interaction. Our assumption is that because of the raw, unedited nature of the videos used in wePorter, disregarding the final few seconds of videos will be tolerable. People recording video in a point and shoot fashion usually stop recording when a phenomenon that prompted them to start filming has ended and so the final bit of their videos does not commonly contain the most important content.

\begin{algorithm}
  \caption{Generate Video Parts}
  \begin{algorithmic}[1]
    \Procedure{Partition}{$sourceVideos,d$}\Comment{partition videos into parts with duration $d$}
      \State $i\gets 1$
      \ForAll{$video \in sourceVideos$} 
        \State $tc_{in} \gets 0$
        % \State $length \gets src.length$
        \While{$tc_{in} \le video.length - d$}
          \State $tc_{out} \gets tc_{in}+d$
          \State $videoPart_i \gets (video.src, tc_{in}, tc_{out})$ 
          \State $tc_{in} \gets tc_{in}+d$
          \State $i \gets i+1$
        \EndWhile
      \EndFor
    \EndProcedure
  \end{algorithmic}
  \label{alg:videoparts}
\end{algorithm}

\subsection{System Framework}

% start = system defined query result set
Figure \ref{fig:framework} shows the data framework of the wePorter system. The system runs as a web interface and is accessible online for multiple users at the same time. The server-side functionality is implemented in PHP making use of connections to a mySQL database. On the client-side, Javascript deals with the playback of video sequences as well as keeping track of all interaction data. Upon a user's navigation to the wePorter web page, two sequences are loaded in the parallel video player. A user triggers the interaction by clicking a play button. During the interaction, Javascript running in the user's browser keeps track of the accumulating attentional ratings per video part. Once an interaction has finished, all interaction data is added to the database on the server. The updated interaction data and counts of video parts are subsequently used in the generation of sequences for new interactions either by the same user or a new visitor.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=\textwidth]{img/framework}
  \caption{System Framework of wePorter}
  \label{fig:framework}
\end{figure}

% single set of videos
For experimentation purposes, the current implementation maintains a single set of topically related source videos representing the context of `encapsulated wander' for a single query. An extension of the system would be to let users query a live database like YouTube for content of their interest and thus define a dynamic collection of content to explore. Another option would be to give users a choice of which set of videos they would like to interact with. These methods could prove to be useful as they let users interact with content they have chosen themselves, which might make their interaction more interest-driven. In our current implementation, we have chosen to use a fixed set of video content for more controllable experimentation, and left these extensions for future work.


\subsubsection{Loading Sequences}
% sequences satisfy 2 constraints
The procedure of loading two sequences for a user interactions is detailed in algorithm \ref{alg:genseqs}. Given the set of video parts, we iteratively construct two sequences of $n\_parts$ video parts to be played in parallel. The sequences have equal length and satisfy the following constraints:

\begin{enumerate}
  \item \textbf{Horizontal source constraint:} Video parts within a sequence all originate from different source videos.
  \item \textbf{Vertical source constraint:} Two video parts that are played concurrently one above the other, originate from different source videos.
\end{enumerate}

% constraints ensure variety
These constraints guarantee variety in the interaction, both within a single sequence and across sequences for concurrent parts. On one hand it enables a more varied editing of the video story, which is desired for the user experience. On the other hand it makes sure that each interaction elicits user preference for a variety of sources, which leads to diversified data acquisition.

% algorithm
\begin{algorithm}
  \caption{Load Sequences Random Shuffled}
  \begin{algorithmic}[1]
    \Procedure{Load Sequences}{$n\_parts, videoParts, vpCounts$}
      \State $seq_1 \gets []$
      \State $seq_2 \gets []$
      \For{$i \gets 0, n\_parts$}
        \State $selectionH_1 \gets []$\Comment{keep selections of parts that satisfy constraints}
        \State $selectionH_2 \gets []$
        \ForAll{$vp \in videoParts$}\Comment{Horizontal constraint}
          \If{$vp.vIdx \neq part.vIdx$ \textbf{for all} $part \in seq_1$}
            \State add $vp$ to $selectionH_1$
          \EndIf
          \If{$vp.vIdx \neq part.vIdx$ \textbf{for all} $part \in seq_2$}
            \State add $vp$ to $selectionH_2$
          \EndIf
        \EndFor
        
        \Comment{For $seq_1$:}
        \State $minSelection_1 \gets []$\Comment{select from $selectionH_1$ parts that have minimal count}
        \State $minCount_1 \gets min(count)$ from $selectionH_1$\Comment{look up counts in $vpCounts$}
        \ForAll{$vp \in selection_1$}
          \If{$vp.count = minCount_1$}
            \State add $vp$ to $minSelection_1$
          \EndIf
        \EndFor
        \State $selected_1 \gets$ random from $minSelection_1$
        \State append $selected_1$ to $seq1$\Comment{add video part to $seq_1$}
        
        \Comment{For $seq_2$:}
        % \Comment{select from $selectionH_2$ parts with different src than $selected_1$}
        \State $selectionV = []$
        \ForAll{$vp \in selection_2$} \Comment{Vertical constraint}
          \If{$vp.src \neq selected_1.src$}
            \State add $vp$ to $selectionV$
          \EndIf
        \EndFor
        
        \State $minSelection_2 \gets []$\Comment{select from $selectionV$ parts that have minimal count}
        \State $minCount_2 \gets min(count)$ from $selectionV$\Comment{look up counts in $vpCounts$}
        \ForAll{$vp \in selection_2$}
          \If{$vp.count = minCount_2$}
            \State add $vp$ to $minSelection_2$
          \EndIf
        \EndFor
        \State $selected_2 \gets$ random from $minSelection_2$
        \State append $selected_2$ to $seq2$\Comment{add video part to $seq_2$}
      \EndFor
      \State $shuffle\_in\_unison(seq_1, seq_1)$
      \State return $(seq_1, seq_2)$
    \EndProcedure
  \end{algorithmic}
  \label{alg:genseqs}
\end{algorithm}

Amongst the interaction data that is stored in the database, we keep a count for every video part of how many times it has been presented. The counts are used to select from the set of video parts that satisfy the constraints, the ones that are least presented so far. This ensures that all video parts will be presented roughly an equal number of times.

After the two sequences have reached the desired size of $n\_parts$, they are randomly shuffled to make sure video parts are presented at different positions in sequence roughly equal amounts of time. Shuffling happens in unison which means correspondence is kept across both sequences so that the constraints still hold. The process of shuffling in unison is described in pseudo code in algorithm \ref{alg:shuffle}.

% shuffle algorith
\begin{algorithm}
  \caption{Shuffle Sequences in Unison}
  \begin{algorithmic}[1]
    \Procedure{Shuffle in unison}{$seq_1, seq_2$}
      \State $seed \gets make\_seed()$ \Comment{to seed $random\_generator$ twice identically}
      \State $random\_generator.set\_seed(seed)$ \Comment{set seed}
      \State $random\_generator.shuffle(seq_1)$ \Comment{shuffle}
      \State $random\_generator.set\_seed(seed)$ \Comment{set seed}
      \State $random\_generator.shuffle(seq_2)$ \Comment{shuffle}
      \State return $(seq_1, seq_2)$
    \EndProcedure
  \end{algorithmic}
  \label{alg:shuffle}
\end{algorithm}

[TODO comment on combinatorics and complexity. all video parts are presented more or less equal, except long videos. for n videos, how many iterations would be needed to converge to optimum of k... convergence has Log complexity]

\subsection{Hypervideo in a Web Browser}

Playing back parts of different online videos in a single video experience is a common feature of any video editing system, but this functionality has only recently become available to code that runs in a web browser. Today most videos that live on the web are much like black boxes and this is not just because computers are having a hard time understanding the visuals. When we interact with video online it is almost always on the high level of the entire video. Whether it's playing, sharing, commenting on or linking to video, we lack the functionality of referring to parts that lie within or interact with components like (sub)titles, images or audio as separate entities. [TODO ... ref video vortex] calls this type of video ``hard'' and ``flat'' [TODO check quotes].

This is starting to change. An important player in the movement of treating video like the web by hyperlinking, cross referencing and remixing it in code, is Mozilla, whose foundation runs `Popcorn' \footnote{\url{http://mozillapopcorn.org/}}, a project that makes video work much like the web. Part of the Popcorn project is `Popcorn.js' \footnote{\url{http://popcornjs.org/}}, a Javascript library that intends to open up videos to the web, give it back it's depth and make them more soft and reconfigurable. The wePorter system relies for a big part on the `Popcorn.js' library for the playback of video parts in sequence.

% ? player can handle videos of different dimensions
